# Проверка настроек robots.txt

Robots.txt — текстовый файл, который содержит параметры индексирования сайта для роботов поисковых систем.

## Как создать robots.txt

В текстовом редакторе создайте файл с именем robots.txt и заполните его в соответствии с представленными ниже правилами.
Проверьте файл в сервисе Яндекс.Вебмастер (пункт меню Анализ robots.txt) или Google Webmaster tools.
Загрузите файл в корневую директорию вашего сайта.

Чтобы запретить доступ робота к сайту или некоторым его разделам, используйте директиву Disallow.

Чтобы разрешить доступ робота к сайту или некоторым его разделам, используйте директиву Allow

Примеры User-agent в robots.txt:
# Указывает директивы для всех роботов одновременно
User-agent: *

# Указывает директивы для всех роботов Яндекса
User-agent: Yandex

# Указывает директивы для только основного индексирующего робота Яндекса
User-agent: YandexBot

# Указывает директивы для всех роботов Google
User-agent: Googlebot

В соответствии со стандартом перед каждой директивой User-agent рекомендуется вставлять пустой перевод строки.

user-agent: a
disallow: /c

user-agent: b
disallow: /d

user-agent: e
user-agent: f
disallow: /g

Символ # предназначен для описания комментариев. Все, что находится после этого символа и до первого перевода строки не учитывается.


Host: применяется для указание Яндексу основного зеркала сайта.


